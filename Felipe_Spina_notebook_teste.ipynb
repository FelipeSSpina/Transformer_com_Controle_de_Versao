{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViwK0RJUtprr",
        "outputId": "472fc033-1d3c-4177-a7cb-eb517d2050b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n",
            "Mixed precision policy: <DTypePolicy \"mixed_float16\">\n",
            "GPUs visíveis: []\n"
          ]
        }
      ],
      "source": [
        "# CÉLULA 1 — Setup de pacotes + Mixed Precision (rápido e estável)\n",
        "import sys, subprocess, importlib\n",
        "\n",
        "def pip_install(pkg: str):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Pacotes utilitários\n",
        "for pkg in [\"tensorflow-datasets\", \"sacrebleu\"]:\n",
        "    try:\n",
        "        importlib.import_module(pkg.replace(\"-\", \"_\"))\n",
        "    except Exception:\n",
        "        pip_install(pkg)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# tensorflow-text precisa casar com a versão do TF\n",
        "try:\n",
        "    import tensorflow_text as text  # noqa\n",
        "except Exception:\n",
        "    pip_install(f\"tensorflow-text=={tf.__version__}\")\n",
        "    import tensorflow_text as text  # noqa\n",
        "\n",
        "# Mixed precision: acelera em GPUs com Tensor Cores (T4/A100)\n",
        "from tensorflow.keras import mixed_precision as mp\n",
        "mp.set_global_policy(\"mixed_float16\")\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"Mixed precision policy:\", mp.global_policy())\n",
        "print(\"GPUs visíveis:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 2 — Imports base, seeds e TFDS (TED HRLR pt→en)\n",
        "import time, numpy as np, tensorflow_datasets as tfds\n",
        "np.random.seed(42); tf.random.set_seed(42)\n",
        "\n",
        "examples, info = tfds.load(\"ted_hrlr_translate/pt_to_en\", with_info=True, as_supervised=True)\n",
        "train_ds, val_ds, test_ds = examples[\"train\"], examples[\"validation\"], examples[\"test\"]\n",
        "print(info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snbpL3sWtrfM",
        "outputId": "039274a9-3ddb-45cd-a99a-bdc4610ef647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='ted_hrlr_translate',\n",
            "    full_name='ted_hrlr_translate/pt_to_en/1.0.0',\n",
            "    description=\"\"\"\n",
            "    Data sets derived from TED talk transcripts for comparing similar language pairs\n",
            "    where one is high resource and the other is low resource.\n",
            "    \"\"\",\n",
            "    config_description=\"\"\"\n",
            "    Translation dataset from pt to en in plain text.\n",
            "    \"\"\",\n",
            "    homepage='https://github.com/neulab/word-embeddings-for-nmt',\n",
            "    data_dir='/root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=124.94 MiB,\n",
            "    dataset_size=10.89 MiB,\n",
            "    features=Translation({\n",
            "        'en': Text(shape=(), dtype=string),\n",
            "        'pt': Text(shape=(), dtype=string),\n",
            "    }),\n",
            "    supervised_keys=('pt', 'en'),\n",
            "    disable_shuffling=False,\n",
            "    nondeterministic_order=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=1803, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=51785, num_shards=1>,\n",
            "        'validation': <SplitInfo num_examples=1193, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@inproceedings{Ye2018WordEmbeddings,\n",
            "      author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n",
            "      title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n",
            "      booktitle = {HLT-NAACL},\n",
            "      year    = {2018},\n",
            "      }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 3 — (opcional) Amostra de dados bruta\n",
        "for pt, en in train_ds.batch(3).take(1):\n",
        "    print(\"PT:\", [x.decode() for x in pt.numpy()])\n",
        "    print(\"EN:\", [x.decode() for x in en.numpy()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5YUWRl9trhe",
        "outputId": "6a716463-b63a-4120-b9bd-6d57da406009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PT: ['e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .', 'mas e se estes fatores fossem ativos ?', 'mas eles não tinham a curiosidade de me testar .']\n",
            "EN: ['and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', 'but what if it were active ?', \"but they did n't test for curiosity .\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 2A — Sanity check de GPU (rode antes do treino)\n",
        "import os, tensorflow as tf, time, numpy as np\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"Logical GPUs:\", tf.config.list_logical_devices('GPU'))\n",
        "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"<não definido>\"))\n",
        "\n",
        "# micro-teste de matmul p/ ver tempo de compute\n",
        "x = tf.random.normal([4096, 4096])\n",
        "t0 = time.perf_counter()\n",
        "_ = tf.linalg.matmul(x, x)  # deve rodar no GPU se disponível\n",
        "dt = time.perf_counter() - t0\n",
        "print(f\"Teste matmul 4096x4096: {dt:.3f}s (GPU deve ser << CPU)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS7fOSsb3uWx",
        "outputId": "04474cad-a810-4a78-cb10-885514ad7d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n",
            "Logical GPUs: []\n",
            "CUDA_VISIBLE_DEVICES: <não definido>\n",
            "Teste matmul 4096x4096: 3.595s (GPU deve ser << CPU)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 4 — Tokenizers TED (carregamento robusto do SavedModel)\n",
        "import os, zipfile, pathlib, shutil\n",
        "\n",
        "model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    f\"{model_name}.zip\",\n",
        "    f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n",
        "    cache_dir='.', cache_subdir='', extract=False)\n",
        "\n",
        "extract_dir = pathlib.Path(zip_path).with_suffix('')  # ./ted_hrlr_translate_pt_en_converter\n",
        "if extract_dir.exists():\n",
        "    shutil.rmtree(extract_dir)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(path=extract_dir)\n",
        "\n",
        "candidates = [extract_dir, extract_dir / model_name]\n",
        "load_dir = next((c for c in candidates if (c / \"saved_model.pb\").exists()), None)\n",
        "if load_dir is None:\n",
        "    raise FileNotFoundError(f\"SavedModel não encontrado em {candidates}\")\n",
        "\n",
        "print(\"Carregando tokenizers de:\", load_dir.resolve())\n",
        "tokenizers = tf.saved_model.load(str(load_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UEVffj1trjp",
        "outputId": "0462a10f-47f3-4de1-d7ef-7c1c2f4d0f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando tokenizers de: /content/ted_hrlr_translate_pt_en_converter/ted_hrlr_translate_pt_en_converter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 5 — Pipeline ultra-rápido (≈10 min)\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE  = 32         # ↓ batch menor reduz custo/step\n",
        "MAX_TOKENS  = 32         # ↓ seq curta acelera bastante\n",
        "\n",
        "def prepare_batch(pt, en):\n",
        "    # tokeniza e corta\n",
        "    pt_tok = tokenizers.pt.tokenize(pt)[:, :MAX_TOKENS].to_tensor()\n",
        "    en_tok = tokenizers.en.tokenize(en)[:, :MAX_TOKENS+1]\n",
        "    # pares (in/label)\n",
        "    en_in  = en_tok[:, :-1].to_tensor()\n",
        "    en_lab = en_tok[:,  1:].to_tensor()\n",
        "    return (pt_tok, en_in), en_lab\n",
        "\n",
        "def make_batches(ds):\n",
        "    return (ds\n",
        "            .shuffle(BUFFER_SIZE)\n",
        "            .batch(BATCH_SIZE, drop_remainder=True)  # drop_remainder evita shapes variando (menos retrace)\n",
        "            .map(prepare_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "train_batches = make_batches(train_ds)\n",
        "val_batches   = make_batches(val_ds)\n"
      ],
      "metadata": {
        "id": "-FU0rSDItrlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 6 — Positional Encoding (sen/cos)\n",
        "def positional_encoding(length, depth):\n",
        "    import numpy as np\n",
        "    depth = depth/2\n",
        "    positions = np.arange(length)[:, None]\n",
        "    depths    = np.arange(depth)[None, :]/depth\n",
        "    angle_rates = 1/(10000**depths)\n",
        "    angle_rads  = positions * angle_rates\n",
        "    pe = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1).astype(\"float32\")\n",
        "    return tf.constant(pe)  # tf.float32\n"
      ],
      "metadata": {
        "id": "vrMFAD7ytrn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 7 — Blocos de atenção/FFN (com dtypes consistentes em MP)\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, heads, d_model, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=d_model, dropout=drop)\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.norm = tf.keras.layers.LayerNormalization(dtype='float32')  # LN em fp32 aumenta estabilidade\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        y = self.mha(query=x, value=x, key=x)\n",
        "        y = tf.cast(y, x.dtype)  # garante dtype igual antes do Add\n",
        "        x = self.add([x, y])\n",
        "        return self.norm(x)\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        y = self.mha(query=x, value=x, key=x, use_causal_mask=True)\n",
        "        y = tf.cast(y, x.dtype)\n",
        "        x = self.add([x, y])\n",
        "        return self.norm(x)\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "    def call(self, x, ctx):\n",
        "        y, _ = self.mha(query=x, value=ctx, key=ctx, return_attention_scores=True)\n",
        "        y = tf.cast(y, x.dtype)\n",
        "        x = self.add([x, y])\n",
        "        return self.norm(x)\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(dff, activation=\"relu\")\n",
        "        self.d2 = tf.keras.layers.Dense(d_model)\n",
        "        self.do = tf.keras.layers.Dropout(drop)\n",
        "        self.ln = tf.keras.layers.LayerNormalization(dtype='float32')\n",
        "    def call(self, x):\n",
        "        y = self.d1(x)\n",
        "        y = self.d2(y)\n",
        "        y = tf.cast(y, x.dtype)\n",
        "        y = self.do(y)\n",
        "        y = self.add = tf.keras.layers.Add()([x, y])\n",
        "        return self.ln(y)\n"
      ],
      "metadata": {
        "id": "8_RfJGcjtrqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 8 — Encoder, Decoder e Transformer (cast da PE + logits em fp32)\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, heads, dff, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.sa = GlobalSelfAttention(heads, d_model, drop)\n",
        "        self.ff = FeedForward(d_model, dff, drop)\n",
        "    def call(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ff(x)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, heads, dff, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.sa = CausalSelfAttention(heads, d_model, drop)\n",
        "        self.ca = CrossAttention(heads, d_model, drop)\n",
        "        self.ff = FeedForward(d_model, dff, drop)\n",
        "    def call(self, x, ctx):\n",
        "        x = self.sa(x)\n",
        "        x = self.ca(x, ctx)\n",
        "        x = self.ff(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, L, d_model, heads, dff, vocab, drop=0.1, max_pos=2048):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(vocab, d_model, mask_zero=True)\n",
        "        self.pe  = positional_encoding(max_pos, d_model)  # float32\n",
        "        self.layers = [EncoderLayer(d_model, heads, dff, drop) for _ in range(L)]\n",
        "    def call(self, x):\n",
        "        x = self.emb(x)  # provavelmente float16 sob MP\n",
        "        pe = tf.cast(self.pe[None, :tf.shape(x)[1], :], x.dtype)  # CAST → dtype do embedding\n",
        "        x = x + pe\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, L, d_model, heads, dff, vocab, drop=0.1, max_pos=2048):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(vocab, d_model, mask_zero=True)\n",
        "        self.pe  = positional_encoding(max_pos, d_model)\n",
        "        self.layers = [DecoderLayer(d_model, heads, dff, drop) for _ in range(L)]\n",
        "        self.out = tf.keras.layers.Dense(vocab, dtype='float32')  # logits em fp32\n",
        "    def call(self, x, ctx):\n",
        "        x = self.emb(x)\n",
        "        pe = tf.cast(self.pe[None, :tf.shape(x)[1], :], x.dtype)\n",
        "        x = x + pe\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, ctx)\n",
        "        try:\n",
        "            del x._keras_mask  # remove máscara herdada do Embedding\n",
        "        except Exception:\n",
        "            pass\n",
        "        return self.out(x)\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, L, d_model, heads, dff, src_vocab, tgt_vocab, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.enc = Encoder(L, d_model, heads, dff, src_vocab, drop)\n",
        "        self.dec = Decoder(L, d_model, heads, dff, tgt_vocab, drop)\n",
        "    def call(self, inputs):\n",
        "        pt, en = inputs\n",
        "        ctx = self.enc(pt)\n",
        "        return self.dec(en, ctx)\n"
      ],
      "metadata": {
        "id": "TkR-vlIGtrr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 9 — Modelo “míni” e compile\n",
        "def masked_loss(y_true, y_pred):\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), y_pred.dtype)\n",
        "    loss = loss * tf.cast(mask, loss.dtype)\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    pred = tf.argmax(y_pred, axis=-1, output_type=y_true.dtype)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), y_true.dtype)\n",
        "    match = tf.cast(tf.equal(y_true, pred), y_true.dtype) * mask\n",
        "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=1000):\n",
        "        super().__init__(); self.d_model=tf.cast(d_model, tf.float32); self.warmup=warmup_steps\n",
        "    def __call__(self, step):\n",
        "        step=tf.cast(step, tf.float32)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(tf.math.rsqrt(step), step*(self.warmup**-1.5))\n",
        "\n",
        "# hiperparâmetros míni (1 camada, largura baixa)\n",
        "L, d_model, dff, heads, drop = 1, 64, 128, 4, 0.1   # heads divide d_model (64/4=16)\n",
        "\n",
        "transformer = Transformer(\n",
        "    L, d_model, heads, dff,\n",
        "    tokenizers.pt.get_vocab_size().numpy(),\n",
        "    tokenizers.en.get_vocab_size().numpy(),\n",
        "    drop\n",
        ")\n",
        "\n",
        "lr  = CustomSchedule(d_model)\n",
        "opt = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "transformer.compile(optimizer=opt, loss=masked_loss, metrics=[masked_accuracy])\n",
        "transformer.build([(None, None), (None, None)])\n",
        "print(transformer.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "EInEMmhbtrtk",
        "outputId": "4e286cd5-f146-4421-dfb5-07788896add8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 10A — Warm-up (compila grafo e aquece pipeline) — roda UMA vez antes do fit real\n",
        "# usa 5 batches de treino e 2 de validação, silencioso (verbose=0)\n",
        "_ = transformer.fit(\n",
        "    train_batches.take(5),\n",
        "    epochs=1,\n",
        "    steps_per_epoch=5,\n",
        "    validation_data=val_batches.take(2),\n",
        "    validation_steps=2,\n",
        "    verbose=0,\n",
        ")\n",
        "print(\"Warm-up concluído.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax2Hg84Q3iSH",
        "outputId": "a28968da-c72e-47c6-d4fb-6751c894b67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warm-up concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 10 — Treino ultra-rápido (≈10 min total, muitas vezes menos)\n",
        "import time\n",
        "\n",
        "class EpochTimer(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None): self.times=[]\n",
        "    def on_epoch_begin(self, epoch, logs=None): self.t0=time.perf_counter()\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        dt=time.perf_counter()-self.t0; self.times.append(dt)\n",
        "        print(f\"[tempo] época {epoch+1}: {dt/60:.2f} min | loss={logs.get('loss'):.4f} | val_loss={logs.get('val_loss'):.4f}\")\n",
        "\n",
        "EPOCHS = 1\n",
        "STEPS_PER_EPOCH = 5    # 5 steps apenas\n",
        "VAL_STEPS       = 2\n",
        "\n",
        "hist = transformer.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_data=val_batches,\n",
        "    validation_steps=VAL_STEPS,\n",
        "    callbacks=[EpochTimer()]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTg901IFtrvX",
        "outputId": "a226c4cb-1015-43e3-91bf-d6aac25f3acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 8.8748 - masked_accuracy: 0.0000e+00[tempo] época 1: 0.23 min | loss=8.8749 | val_loss=8.8708\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - loss: 8.8748 - masked_accuracy: 0.0000e+00 - val_loss: 8.8708 - val_masked_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 11 — Tradução greedy (rápida)\n",
        "probe = tokenizers.en.tokenize(tf.constant([\"hello\"]))\n",
        "START_ID = int(probe[0][0].numpy())\n",
        "END_ID   = int(probe[0][-1].numpy())\n",
        "MAX_GEN  = 32   # coerente com MAX_TOKENS\n",
        "\n",
        "def translate_pt2en(sentence_pt: str):\n",
        "    pt = tf.constant([sentence_pt])\n",
        "    pt_tokens = tokenizers.pt.tokenize(pt).to_tensor()\n",
        "    en_tokens = tf.constant([[START_ID]], dtype=tf.int64)\n",
        "    for _ in range(MAX_GEN):\n",
        "        logits = transformer((pt_tokens, en_tokens))\n",
        "        next_id = tf.argmax(logits[:, -1, :], axis=-1, output_type=en_tokens.dtype)\n",
        "        en_tokens = tf.concat([en_tokens, next_id[:, None]], axis=1)\n",
        "        if int(next_id[0].numpy()) == END_ID: break\n",
        "    return tokenizers.en.detokenize(en_tokens)[0].numpy().decode(\"utf-8\")\n",
        "\n",
        "print(translate_pt2en(\"este é o primeiro livro que eu fiz.\"))\n",
        "print(translate_pt2en(\"gostaria de um copo de água, por favor.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsAyQQJ2t2Ql",
        "outputId": "95d6e5ff-5acc-4def-e2ee-d6e5f32bdf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theme rape sake sake sakeitchitchitchitchitchitchitchitchitchitch estimate psychologist psychologist psychologist equivalent lighting directly directly hardware device device johnnyitchitchitchitch bees\n",
            "theme hundred sake sake sakeitchitchitchitchitchitchitchitchitchitch estimate psychologist psychologist psychologist equivalent lighting directly directly hardware device device johnnyitch inchesitchitch bees\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 12 — (opcional) SacreBLEU rápido em amostra pequena\n",
        "import sacrebleu as sbl\n",
        "\n",
        "def bleu_on(ds, n=32):\n",
        "    hyps, refs = [], []\n",
        "    for pt, en in ds.unbatch().take(n):\n",
        "        pt_s, en_s = pt.numpy().decode(\"utf-8\"), en.numpy().decode(\"utf-8\")\n",
        "        hyp = translate_pt2en(pt_s)\n",
        "        hyps.append(hyp)\n",
        "        refs.append([en_s])\n",
        "    return sbl.corpus_bleu(hyps, list(zip(*refs))).score\n",
        "\n",
        "# Descomente para medir:\n",
        "# bleu_val = bleu_on(val_ds, n=32)\n",
        "# print(\"SacreBLEU (val, n=32):\", bleu_val)\n"
      ],
      "metadata": {
        "id": "j5g_Cla7trxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}